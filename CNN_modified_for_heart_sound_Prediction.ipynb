{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jjsnf43wnd7b0vrssFDsEZOVBg9YB-BM",
      "authorship_tag": "ABX9TyOvxw6p8cjU9xPr2GW9c5XS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shatisvigian/Big-Data-learning-Train-Scheduling/blob/main/CNN_modified_for_heart_sound_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Cz1uICBW3yNb",
        "outputId": "122d3d92-cf5f-4e56-eccb-7ed2c8c9d74e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ae9ed68b-cb13-40e4-9096-21a76369ffae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ae9ed68b-cb13-40e4-9096-21a76369ffae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving a0007.zip to a0007.zip\n",
            "['a0007.wav']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Upload the dataset zip file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the uploaded zip file\n",
        "zip_file_name = list(uploaded.keys())[0]\n",
        "zip_file_path = os.path.join('/content', zip_file_name)\n",
        "extracted_path = '/content/extracted_data/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "# List the contents of the extracted directory\n",
        "extracted_contents = os.listdir(extracted_path)\n",
        "print(extracted_contents)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import (Sequential, Model)\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming your prep module and get_train_test function are available\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049,1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese') :\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs = xc, outputs = x)\n",
        "    return model\n",
        "\n",
        "def generator(x,y,batch_size=10):\n",
        "    i = 0\n",
        "    while(True):\n",
        "        if i+batch_size >= y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = list()\n",
        "        y_batch = list()\n",
        "        for j in range(i,i+batch_size):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i+batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch,np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer = keras.optimizers.Adam(), loss = keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = y_train.shape[0]//batch_size\n",
        "    validation_steps = y_test.shape[0]//batch_size\n",
        "\n",
        "    train = generator(x_train,y_train,batch_size=batch_size)\n",
        "    test = generator(x_test,y_test,batch_size=batch_size)\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelSCNN.hdf5', verbose=1, save_best_only=True)\n",
        "    else:\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelReSE.hdf5', verbose=1, save_best_only=True)\n",
        "    out.fit_generator(train,steps_per_epoch=steps_per_epoch,epochs=10,validation_data=test,validation_steps=validation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "4DkPqUwR57Ar",
        "outputId": "369ec66f-ccb6-4a76-ada1-5ccf26b403b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-7-724b12f5b2ad>, line 157)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-724b12f5b2ad>\"\u001b[0;36m, line \u001b[0;32m157\u001b[0m\n\u001b[0;31m    out.fit_generator(train,steps_per_epoch=steps_per_epoch,epochs=10,validation_data=test,validation_steps=validation\u001b[0m\n\u001b[0m                                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iY0ZMh6U8cZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import (Sequential, Model)\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming your prep module and get_train_test function are available\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049,1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese') :\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs = xc, outputs = x)\n",
        "    return model\n",
        "\n",
        "def generator(x,y,batch_size=10):\n",
        "    i = 0\n",
        "    while(True):\n",
        "        if i+batch_size >= y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = list()\n",
        "        y_batch = list()\n",
        "        for j in range(i,i+batch_size):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i+batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch,np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer = keras.optimizers.Adam(), loss = keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = y_train.shape[0]//batch_size\n",
        "    validation_steps = y_test.shape[0]//batch_size\n",
        "\n",
        "    train = generator(x_train,y_train,batch_size=batch_size)\n",
        "    test = generator(x_test,y_test,batch_size=batch_size)\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelSCNN.hdf5', verbose=1, save_best_only=True)\n",
        "    else:\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelReSE.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "    out.fit(train, steps_per_epoch=steps_per_epoch, epochs=10, validation_data=test, validation_steps=validation_steps, callbacks=[checkpointer])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "3XZMB3fv6foK",
        "outputId": "6fb1c467-388f-4ccc-95f3-298310d83629"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What model do you want to train? \n",
            "Type 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\n",
            "rese\n",
            "Building a ReSE-2-Multi model.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'prep' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-98b36480f98d>\u001b[0m in \u001b[0;36m<cell line: 160>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-98b36480f98d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown model type: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnet_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'prep' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Upload the dataset\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the dataset\n",
        "for file_name in uploaded.keys():\n",
        "    if zipfile.is_zipfile(file_name):\n",
        "        with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content/extracted_data')\n",
        "        print(\"Extraction successful!\")\n",
        "    else:\n",
        "        print(f\"{file_name} is not a valid zip file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "YUHaL-uW8erz",
        "outputId": "2d7df21a-aa54-4428-f713-99fe059f684d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7c28dc5b-165a-4dd1-abb8-57afc9d4f491\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7c28dc5b-165a-4dd1-abb8-57afc9d4f491\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving a0007.zip to a0007 (1).zip\n",
            "Extraction successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n"
      ],
      "metadata": {
        "id": "iTqUoxFb8jHS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049,1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese') :\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs = xc, outputs = x)\n",
        "    return model\n",
        "\n",
        "def generator(x,y,batch_size=10):\n",
        "    i = 0\n",
        "    while(True):\n",
        "        if i+batch_size >= y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = list()\n",
        "        y_batch = list()\n",
        "        for j in range(i,i+batch_size):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i+batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch,np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "MrCbGHo98oxx",
        "outputId": "11b3ae29-f3b0-467e-b42b-7e109b79eb46"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 153) (<ipython-input-12-8d292a81fe12>, line 153)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-8d292a81fe12>\"\u001b[0;36m, line \u001b[0;32m153\u001b[0m\n\u001b[0;31m    net_type = input(\"What model do you want to train? \\nType\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 153)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049,1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese') :\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs = xc, outputs = x)\n",
        "    return model\n",
        "\n",
        "def generator(x, y, batch_size=10):\n",
        "    i = 0\n",
        "    while(True):\n",
        "        if i + batch_size >= y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = list()\n",
        "        y_batch = list()\n",
        "        for j in range(i, i + batch_size):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i + batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch, np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer = keras.optimizers.Adam(), loss = keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = y_train.shape[0] // batch_size\n",
        "    validation_steps = y_test.shape[0] // batch_size\n",
        "\n",
        "    train = generator(x_train, y_train, batch_size=batch_size)\n",
        "    test = generator(x_test, y_test, batch_size=batch_size)\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelSCNN.hdf5', verbose=1, save_best_only=True)\n",
        "    else:\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelReSE.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "    out.fit(train, steps_per_epoch=steps_per_epoch, epochs=10, validation_data=test, validation_steps=validation_steps, callbacks=[checkpointer])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "UVX5WbNr9Nly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5emqopTIKlq",
        "outputId": "e77e8216-6f40-4265-e99b-8dfd67fa7da9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Upload the dataset zip file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the uploaded zip file\n",
        "zip_file_name = list(uploaded.keys())[0]\n",
        "zip_file_path = os.path.join('/content', zip_file_name)\n",
        "extracted_path = '/content/extracted_data/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "# List the contents of the extracted directory\n",
        "extracted_contents = os.listdir(extracted_path)\n",
        "print(extracted_contents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "S6YbKsBidSiC",
        "outputId": "9957151d-6c5c-468a-b6b8-81fb2b7cd720"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7b6a37a6-c94f-4f19-b470-ea0ec87a2623\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7b6a37a6-c94f-4f19-b470-ea0ec87a2623\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving healthy.zip to healthy.zip\n",
            "['a0007.wav', 'healthy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049,1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese') :\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs = xc, outputs = x)\n",
        "    return model\n",
        "\n",
        "def generator(x, y, batch_size=10):\n",
        "    i = 0\n",
        "    while(True):\n",
        "        if i + batch_size >= y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = list()\n",
        "        y_batch = list()\n",
        "        for j in range(i, i + batch_size):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i + batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch, np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer = keras.optimizers.Adam(), loss = keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = y_train.shape[0] // batch_size\n",
        "    validation_steps = y_test.shape[0] // batch_size\n",
        "\n",
        "    train = generator(x_train, y_train, batch_size=batch_size)\n",
        "    test = generator(x_test, y_test, batch_size=batch_size)\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelSCNN.hdf5', verbose=1, save_best_only=True)\n",
        "    else:\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelReSE.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "    out.fit(train, steps_per_epoch=steps_per_epoch, epochs=10, validation_data=test, validation_steps=validation_steps, callbacks=[checkpointer])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "-wDTSlAEfBND",
        "outputId": "28742a77-db08-4f01-b723-f42b349a9d5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What model do you want to train? \n",
            "Type 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\n",
            "rese\n",
            "Building a ReSE-2-Multi model.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-71954dd2da66>\u001b[0m in \u001b[0;36m<cell line: 186>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-71954dd2da66>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown model type: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnet_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-71954dd2da66>\u001b[0m in \u001b[0;36mget_train_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamplifying_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-71954dd2da66>\u001b[0m in \u001b[0;36mget_train_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    if len(x_data) < 2:\n",
        "        raise ValueError(\"Not enough data samples to perform train-test split.\")\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049,1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese') :\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs = xc, outputs = x)\n",
        "    return model\n",
        "\n",
        "def generator(x, y, batch_size=10):\n",
        "    i = 0\n",
        "    while(True):\n",
        "        if i + batch_size >= y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = list()\n",
        "        y_batch = list()\n",
        "        for j in range(i, i + batch_size):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i + batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch, np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer = keras.optimizers.Adam(), loss = keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = max(1, y_train.shape[0] // batch_size)\n",
        "    validation_steps = max(1, y_test.shape[0] // batch_size)\n",
        "\n",
        "    train = generator(x_train, y_train, batch_size=batch_size)\n",
        "    test = generator(x_test, y_test, batch_size=batch_size)\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelSCNN.hdf5', verbose=1, save_best_only=True)\n",
        "    else:\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelReSE.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "    out.fit(train, steps_per_epoch=steps_per_epoch, epochs=10, validation_data=test, validation_steps=validation_steps, callbacks=[checkpointer])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "tZHp0L6-nZuq",
        "outputId": "bdfc1076-695a-4304-db2d-be924e58bdef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What model do you want to train? \n",
            "Type 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\n",
            "basic\n",
            "Building a Sample CNN model.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Not enough data samples to perform train-test split.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1f520f164fec>\u001b[0m in \u001b[0;36m<cell line: 189>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-1f520f164fec>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown model type: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnet_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-1f520f164fec>\u001b[0m in \u001b[0;36mget_train_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamplifying_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-1f520f164fec>\u001b[0m in \u001b[0;36mget_train_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not enough data samples to perform train-test split.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Not enough data samples to perform train-test split."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    if len(x_data) < 2:\n",
        "        print(\"Not enough data samples to perform train-test split.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049,1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese') :\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs = xc, outputs = x)\n",
        "    return model\n",
        "\n",
        "def generator(x, y, batch_size=10):\n",
        "    i = 0\n",
        "    while(True):\n",
        "        if i + batch_size >= y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = list()\n",
        "        y_batch = list()\n",
        "        for j in range(i, i + batch_size):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i + batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch, np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    if x_train is None or x_test is None or y_train is None or y_test is None:\n",
        "        print(\"Insufficient data to proceed with training.\")\n",
        "        return\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer = keras.optimizers.Adam(), loss = keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = max(1, y_train.shape[0] // batch_size)\n",
        "    validation_steps = max(1, y_test.shape[0] // batch_size)\n",
        "\n",
        "    train = generator(x_train, y_train, batch_size=batch_size)\n",
        "    test = generator(x_test, y_test, batch_size=batch_size)\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelSCNN.hdf5', verbose=1, save_best_only=True)\n",
        "    else:\n",
        "        checkpointer = ModelCheckpoint(filepath='bestModelReSE.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "    out.fit(train, steps_per_epoch=steps_per_epoch, epochs=10, validation_data=test, validation_steps=validation_steps, callbacks=[checkpointer])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ornlR8GsoO5L",
        "outputId": "6f651dff-6269-47c6-ea85-24ddc14f01cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What model do you want to train? \n",
            "Type 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\n",
            "rese\n",
            "Building a ReSE-2-Multi model.\n",
            "\n",
            "Not enough data samples to perform train-test split.\n",
            "Insufficient data to proceed with training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    if len(x_data) < 2:\n",
        "        print(\"Not enough data samples to perform train-test split.\")\n",
        "        # Generate dummy data for testing\n",
        "        num_samples = 10\n",
        "        x_data = np.random.rand(num_samples, 59049, 1)\n",
        "        y_data = np.random.randint(0, 12, num_samples)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049,1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese') :\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs = xc, outputs = x)\n",
        "    return model\n",
        "\n",
        "def generator(x, y, batch_size=10):\n",
        "    i = 0\n",
        "    while(True):\n",
        "        if i + batch_size >= y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = list()\n",
        "        y_batch = list()\n",
        "        for j in range(i, i + batch_size):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i + batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch, np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    if x_train is None or x_test is None or y_train is None or y_test is None:\n",
        "        print(\"Insufficient data to proceed with training.\")\n",
        "        return\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer = keras.optimizers.Adam(), loss = keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = max(1, y_train.shape[0] // batch_size)\n",
        "    validation_steps = max(1, y_test.shape[0] // batch_size)\n",
        "\n",
        "    train = generator(x_train, y_train, batch_size=batch_size)\n",
        "    test = generator(x_test, y_test, batch_size=batch_size)\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        checkpointer =\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "pw-ByXPZpE1a",
        "outputId": "05e70964-1b73-44c6-a186-d1a3eeef406b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-9-31e5f23d2f5f>, line 191)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-31e5f23d2f5f>\"\u001b[0;36m, line \u001b[0;32m191\u001b[0m\n\u001b[0;31m    checkpointer =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049,1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese') :\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs = xc, outputs = x)\n",
        "    return model\n",
        "\n",
        "def generator(x, y, batch_size=10):\n",
        "    i = 0\n",
        "    while(True):\n",
        "        if i + batch_size >= y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = list()\n",
        "        y_batch = list()\n",
        "        for j in range(i, i + batch_size):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i + batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch, np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    if x_train is None or x_test is None or y_train is None or y_test is None:\n",
        "        print(\"Insufficient data to proceed with training.\")\n",
        "        return\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer = keras.optimizers.Adam(), loss = keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = max(1, y_train.shape[0] // batch_size)\n",
        "    validation_steps = max(1, y_test.shape[0] // batch_size)\n",
        "\n",
        "    train = generator(x_train, y_train, batch_size=batch_size)\n",
        "    test = generator(x_test, y_test, batch_size=batch_size)\n",
        "\n",
        "    checkpointer = ModelCheckpoint(filepath=f'bestModel{net_type.capitalize()}.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "    out.fit(train, steps_per_epoch=steps_per_epoch, epochs=10, validation_data=test, validation_steps=validation_steps, callbacks=[checkpointer])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "hMWoq0SarAHU",
        "outputId": "f48163d4-091f-486b-eb27-4e331525b588"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 12 (<ipython-input-11-9094ba8db63a>, line 14)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-9094ba8db63a>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    if block_type == 'rese':\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    if len(x_data) < 2:\n",
        "        print(\"Not enough data samples to perform train-test split.\")\n",
        "        # Generate dummy data for testing\n",
        "        num_samples = 10\n",
        "        x_data = np.random.rand(num_samples, 59049, 1)\n",
        "        y_data = np.random.randint(0, 12, num_samples)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049, 1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese'):\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs=xc, outputs=x)\n",
        "    return model\n",
        "\n",
        "def generator(x, y, batch_size=10):\n",
        "    i = 0\n",
        "    while True:\n",
        "        if i + batch_size >= y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = []\n",
        "        y_batch = []\n",
        "        for j in range(i, i + batch_size):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i + batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch, np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    if x_train is None or x_test is None or y_train is None or y_test is None:\n",
        "        print(\"Insufficient data to proceed with training.\")\n",
        "        return\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = max(1, y_train.shape[0] // batch_size)\n",
        "    validation_steps = max(1, y_test.shape[0] // batch_size)\n",
        "\n",
        "    train = generator(x_train, y_train, batch_size=batch_size)\n",
        "    test = generator(x_test, y_test, batch_size=batch_size)\n",
        "\n",
        "    checkpointer = ModelCheckpoint(filepath=f'bestModel{net_type.capitalize()}.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "    out.fit(train, steps_per_epoch=steps_per_epoch, epochs=10, validation_data=test, validation_steps=validation_steps, callbacks=[checkpointer])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V5l1356brsZU",
        "outputId": "7cef76b1-ca1f-4720-f36a-75fadda31f89"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What model do you want to train? \n",
            "Type 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\n",
            "basic\n",
            "Building a Sample CNN model.\n",
            "\n",
            "Not enough data samples to perform train-test split.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 59049, 1)]        0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 59049, 1)          0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 19683, 128)        512       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 19683, 128)        512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " activation (Activation)     (None, 19683, 128)        0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 19683, 128)        49280     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 19683, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 19683, 128)        0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 6561, 128)         0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 6561, 128)         49280     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 6561, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 6561, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 2187, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 2187, 256)         98560     \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 2187, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 2187, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 729, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 729, 256)          196864    \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 729, 256)          1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 729, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPoolin  (None, 243, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 243, 256)          196864    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 243, 256)          1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 243, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPoolin  (None, 81, 256)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 81, 256)           196864    \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 81, 256)           1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 81, 256)           0         \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPoolin  (None, 27, 256)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 27, 256)           196864    \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 27, 256)           1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 27, 256)           0         \n",
            "                                                                 \n",
            " max_pooling1d_6 (MaxPoolin  (None, 9, 256)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 9, 256)            196864    \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 9, 256)            1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 9, 256)            0         \n",
            "                                                                 \n",
            " max_pooling1d_7 (MaxPoolin  (None, 3, 256)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 3, 512)            393728    \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 3, 512)            2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 3, 512)            0         \n",
            "                                                                 \n",
            " max_pooling1d_8 (MaxPoolin  (None, 1, 512)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 512)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               262656    \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 512)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 12)                6156      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1856268 (7.08 MB)\n",
            "Trainable params: 1850380 (7.06 MB)\n",
            "Non-trainable params: 5888 (23.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 8 is out of bounds for axis 0 with size 8",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-94e452a3eea0>\u001b[0m in \u001b[0;36m<cell line: 194>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-94e452a3eea0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'bestModel{net_type.capitalize()}.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-94e452a3eea0>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(x, y, batch_size)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for axis 0 with size 8"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049,1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese') :\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs = xc, outputs = x)\n",
        "    return model\n",
        "\n",
        "def generator(x,y,batch_size=10):\n",
        "    i = 0\n",
        "    while(True):\n",
        "        if i+batch_size >= y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = list()\n",
        "        y_batch = list()\n",
        "        for j in range(i,i+batch_size):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i+batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch,np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "l0ZSeqJ1xwP8",
        "outputId": "ed859996-2363-4d92-cf37-b07a6a32f517"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 153) (<ipython-input-13-8d292a81fe12>, line 153)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-8d292a81fe12>\"\u001b[0;36m, line \u001b[0;32m153\u001b[0m\n\u001b[0;31m    net_type = input(\"What model do you want to train? \\nType\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 153)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    if len(x_data) < 2:\n",
        "        print(\"Not enough data samples to perform train-test split.\")\n",
        "        # Generate dummy data for testing\n",
        "        num_samples = 10\n",
        "        x_data = np.random.rand(num_samples, 59049, 1)\n",
        "        y_data = np.random.randint(0, 12, num_samples)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049, 1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese'):\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs=xc, outputs=x)\n",
        "    return model\n",
        "\n",
        "def generator(x, y, batch_size=10):\n",
        "    i = 0\n",
        "    while True:\n",
        "        if i + batch_size > y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = []\n",
        "        y_batch = []\n",
        "        for j in range(i, min(i + batch_size, y.shape[0])):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i + batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch, np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    if x_train is None or x_test is None or y_train is None or y_test is None:\n",
        "        print(\"Insufficient data to proceed with training.\")\n",
        "        return\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = max(1, y_train.shape[0] // batch_size)\n",
        "    validation_steps = max(1, y_test.shape[0] // batch_size)\n",
        "\n",
        "    train = generator(x_train, y_train, batch_size=batch_size)\n",
        "    test = generator(x_test, y_test, batch_size=batch_size)\n",
        "\n",
        "    checkpointer = ModelCheckpoint(filepath=f'bestModel{net_type.capitalize()}.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "    out.fit(train, steps_per_epoch=steps_per_epoch, epochs=10, validation_data=test, validation_steps=validation_steps, callbacks=[checkpointer])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nCr25PZRzbJB",
        "outputId": "3533fa1a-10fe-4adf-f000-dcdd4e257c29"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What model do you want to train? \n",
            "Type 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\n",
            "basic\n",
            "Building a Sample CNN model.\n",
            "\n",
            "Not enough data samples to perform train-test split.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 59049, 1)]        0         \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 59049, 1)          0         \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 19683, 128)        512       \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 19683, 128)        512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 19683, 128)        0         \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 19683, 128)        49280     \n",
            "                                                                 \n",
            " batch_normalization_12 (Ba  (None, 19683, 128)        512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 19683, 128)        0         \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPoolin  (None, 6561, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_12 (Conv1D)          (None, 6561, 128)         49280     \n",
            "                                                                 \n",
            " batch_normalization_13 (Ba  (None, 6561, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 6561, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPooli  (None, 2187, 128)         0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_13 (Conv1D)          (None, 2187, 256)         98560     \n",
            "                                                                 \n",
            " batch_normalization_14 (Ba  (None, 2187, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 2187, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_11 (MaxPooli  (None, 729, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_14 (Conv1D)          (None, 729, 256)          196864    \n",
            "                                                                 \n",
            " batch_normalization_15 (Ba  (None, 729, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 729, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_12 (MaxPooli  (None, 243, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_15 (Conv1D)          (None, 243, 256)          196864    \n",
            "                                                                 \n",
            " batch_normalization_16 (Ba  (None, 243, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 243, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPooli  (None, 81, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_16 (Conv1D)          (None, 81, 256)           196864    \n",
            "                                                                 \n",
            " batch_normalization_17 (Ba  (None, 81, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 81, 256)           0         \n",
            "                                                                 \n",
            " max_pooling1d_14 (MaxPooli  (None, 27, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_17 (Conv1D)          (None, 27, 256)           196864    \n",
            "                                                                 \n",
            " batch_normalization_18 (Ba  (None, 27, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 27, 256)           0         \n",
            "                                                                 \n",
            " max_pooling1d_15 (MaxPooli  (None, 9, 256)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 9, 256)            196864    \n",
            "                                                                 \n",
            " batch_normalization_19 (Ba  (None, 9, 256)            1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 9, 256)            0         \n",
            "                                                                 \n",
            " max_pooling1d_16 (MaxPooli  (None, 3, 256)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 3, 512)            393728    \n",
            "                                                                 \n",
            " batch_normalization_20 (Ba  (None, 3, 512)            2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 3, 512)            0         \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPooli  (None, 1, 512)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 512)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " batch_normalization_21 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 12)                6156      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1856268 (7.08 MB)\n",
            "Trainable params: 1850380 (7.06 MB)\n",
            "Non-trainable params: 5888 (23.00 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.7668 - accuracy: 0.0000e+00 - f1: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node mul_1 defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-14-2a13b767615e>\", line 195, in <cell line: 194>\n\n  File \"<ipython-input-14-2a13b767615e>\", line 192, in main\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1856, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2296, in evaluate\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 4108, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1920, in test_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 723, in update_state\n\n  File \"<ipython-input-14-2a13b767615e>\", line 155, in f1\n\n  File \"<ipython-input-14-2a13b767615e>\", line 151, in precision\n\nIncompatible shapes: [2,4] vs. [2,12]\n\t [[{{node mul_1}}]] [Op:__inference_test_function_9546]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2a13b767615e>\u001b[0m in \u001b[0;36m<cell line: 194>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-2a13b767615e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'bestModel{net_type.capitalize()}.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node mul_1 defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-14-2a13b767615e>\", line 195, in <cell line: 194>\n\n  File \"<ipython-input-14-2a13b767615e>\", line 192, in main\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1856, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2296, in evaluate\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 4108, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1920, in test_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 723, in update_state\n\n  File \"<ipython-input-14-2a13b767615e>\", line 155, in f1\n\n  File \"<ipython-input-14-2a13b767615e>\", line 151, in precision\n\nIncompatible shapes: [2,4] vs. [2,12]\n\t [[{{node mul_1}}]] [Op:__inference_test_function_9546]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    if len(x_data) < 2:\n",
        "        print(\"Not enough data samples to perform train-test split.\")\n",
        "        # Generate dummy data for testing\n",
        "        num_samples = 10\n",
        "        x_data = np.random.rand(num_samples, 59049, 1)\n",
        "        y_data = np.random.randint(0, 12, num_samples)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049, 1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese'):\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs=xc, outputs=x)\n",
        "    return model\n",
        "\n",
        "def generator(x, y, batch_size=10):\n",
        "    i = 0\n",
        "    while True:\n",
        "        if i + batch_size > y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = []\n",
        "        y_batch = []\n",
        "        for j in range(i, min(i + batch_size, y.shape[0])):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i + batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch, np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "    # Add debugging statements to check shapes\n",
        "    print(f\"y_true shape: {y_true.shape}, y_pred shape: {y_pred.shape}\")\n",
        "\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    if x_train is None or x_test is None or y_train is None or y_test is None:\n",
        "        print(\"Insufficient data to proceed with training.\")\n",
        "        return\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train, num_classes=12)  # Ensure y_train is one-hot encoded correctly\n",
        "    y_test = to_categorical(y_test, num_classes=12)    # Ensure y_test is one-hot encoded correctly\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = max(1, y_train.shape[0] // batch_size)\n",
        "    validation_steps = max(1, y_test.shape[0] // batch_size)\n",
        "\n",
        "    train = generator(x_train, y_train, batch_size=batch_size)\n",
        "    test\n"
      ],
      "metadata": {
        "id": "-ayIrgPY0wAc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import (Conv1D, MaxPool1D, BatchNormalization, GlobalAvgPool1D, Multiply, GlobalMaxPool1D,\n",
        "                          Dense, Dropout, Activation, Reshape, Input, Concatenate, Add)\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as swave\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Example implementation of the get_train_test function\n",
        "def get_train_test():\n",
        "    data_path = '/content/extracted_data/'  # Adjust this path as necessary\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Example data loading logic\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            sample_rate, data = swave.read(os.path.join(data_path, file_name))\n",
        "            data = resize(data, (59049, 1))  # Resize to desired input shape\n",
        "            x_data.append(data)\n",
        "            # Extract label from file_name or other logic\n",
        "            label = int(file_name.split('a')[1].split('.')[0]) % 12  # Example label extraction logic\n",
        "            y_data.append(label)\n",
        "\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    if len(x_data) < 2:\n",
        "        print(\"Not enough data samples to perform train-test split.\")\n",
        "        # Generate dummy data for testing\n",
        "        num_samples = 10\n",
        "        x_data = np.random.rand(num_samples, 59049, 1)\n",
        "        y_data = np.random.randint(0, 12, num_samples)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Define the prep module for simplicity\n",
        "class prep:\n",
        "    @staticmethod\n",
        "    def get_train_test():\n",
        "        return get_train_test()\n",
        "\n",
        "def se_fn(x, amplifying_ratio):\n",
        "    num_features = x.shape[-1]\n",
        "    x = GlobalAvgPool1D()(x)\n",
        "    x = Reshape((1, num_features))(x)\n",
        "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
        "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
        "    return x\n",
        "\n",
        "def basic_block(x, num_features, weight_decay, _):\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def rese_block(x, num_features, weight_decay, amplifying_ratio):\n",
        "    if num_features != x.shape[-1]:\n",
        "        shortcut = Conv1D(num_features, kernel_size=1, padding='same', use_bias=True,\n",
        "                        kernel_regularizer=l2(weight_decay), kernel_initializer='glorot_uniform')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv1D(num_features, kernel_size=3, padding='same', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if amplifying_ratio > 0:\n",
        "        x = Multiply()([x, se_fn(x, amplifying_ratio)])\n",
        "    x = Add()([shortcut, x])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool1D(pool_size=3)(x)\n",
        "    return x\n",
        "\n",
        "def get_model(block_type='basic', multi=True, init_features=128, amplifying_ratio=16,\n",
        "                drop_rate=0.5, weight_decay=0., num_classes=12):\n",
        "    if block_type == 'rese':\n",
        "        block = rese_block\n",
        "    elif block_type == 'basic':\n",
        "        block = basic_block\n",
        "    else:\n",
        "        raise Exception('Unknown block type: ' + block_type)\n",
        "\n",
        "    xc = Input(shape=(59049, 1))\n",
        "    x = Reshape([-1, 1])(xc)\n",
        "\n",
        "    x = Conv1D(init_features, kernel_size=3, strides=3, padding='valid', use_bias=True,\n",
        "                kernel_regularizer=l2(weight_decay), kernel_initializer='he_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    num_features = init_features\n",
        "    layer_outputs = []\n",
        "    for i in range(9):\n",
        "        num_features *= 2 if (i == 2 or i == 8) else 1\n",
        "        x = block(x, num_features, weight_decay, amplifying_ratio)\n",
        "        layer_outputs.append(x)\n",
        "\n",
        "    if (multi) and (block_type == 'rese'):\n",
        "        x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
        "    else:\n",
        "        x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    x = Dense(x.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if drop_rate > 0.:\n",
        "        x = Dropout(drop_rate)(x)\n",
        "    x = Dense(num_classes, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
        "    model = Model(inputs=xc, outputs=x)\n",
        "    return model\n",
        "\n",
        "def generator(x, y, batch_size=10):\n",
        "    i = 0\n",
        "    while True:\n",
        "        if i + batch_size > y.shape[0]:\n",
        "            i = 0\n",
        "        x_batch = []\n",
        "        y_batch = []\n",
        "        for j in range(i, min(i + batch_size, y.shape[0])):\n",
        "            x_batch.append(x[j])\n",
        "            y_batch.append(y[j])\n",
        "        i = i + batch_size\n",
        "        x_batch = np.array(x_batch)\n",
        "        yield x_batch, np.array(y_batch)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "    # Add debugging statements to check shapes\n",
        "    print(f\"y_true shape: {y_true.shape}, y_pred shape: {y_pred.shape}\")\n",
        "\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\n",
        "def main():\n",
        "    net_type = input(\"What model do you want to train? \\nType 'basic' for Sample CNN model and 'rese' for the ReSE-2-Multi model.\\n\")\n",
        "\n",
        "    if net_type == \"basic\":\n",
        "        print(\"Building a Sample CNN model.\\n\")\n",
        "    elif net_type == \"rese\":\n",
        "        print(\"Building a ReSE-2-Multi model.\\n\")\n",
        "    else:\n",
        "        raise Exception('Unknown model type: ' + net_type)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = prep.get_train_test()\n",
        "\n",
        "    if x_train is None or x_test is None or y_train is None or y_test is None:\n",
        "        print(\"Insufficient data to proceed with training.\")\n",
        "        return\n",
        "\n",
        "    out = get_model(block_type=net_type)\n",
        "    out.summary()\n",
        "\n",
        "    out.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.categorical_crossentropy, metrics=['accuracy', f1])\n",
        "\n",
        "    y_train = to_categorical(y_train, num_classes=12)  # Ensure y_train is one-hot encoded correctly\n",
        "    y_test = to_categorical(y_test, num_classes=12)    # Ensure y_test is one-hot encoded correctly\n",
        "\n",
        "    batch_size = 10\n",
        "    steps_per_epoch = max(1, y_train.shape[0] // batch_size)\n",
        "    validation_steps = max(1, y_test.shape[0] // batch_size)\n",
        "\n",
        "    train = generator(x_train, y_train, batch_size=batch_size)\n",
        "    test = generator(x_test, y_test, batch_size=batch_size)\n",
        "\n",
        "    # Train the model\n",
        "    out.fit(train, epochs=10, steps_per_epoch=steps_per_epoch, validation_data=test, validation_steps=validation_steps)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GjIKvf2f1jfO"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}